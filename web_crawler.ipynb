{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Web Crawler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def get_page(url):\n",
    "    try:\n",
    "        return urllib.urlopen(url).read()\n",
    "    except:\n",
    "        return \"\"\n",
    "print(get_page('http://www.gutenberg.org/cache/epub/1661/pg1661.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def union(list1,list2):\n",
    "    for entry in list2:\n",
    "        if entry not in list1:\n",
    "            list1.append(entry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_next_link(page):\n",
    "    start_link = page.find('<a href=')\n",
    "    \n",
    "    if startpos == -1:\n",
    "        return None, 0\n",
    "    \n",
    "    start_quote = page.find('\"', start_link)\n",
    "    end_quote = page.find('\"', start_quote + 1)\n",
    "    url = page[start_quote+1:end_quote]\n",
    "    \n",
    "    return url, end_quote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_links(page):\n",
    "    links = []\n",
    "    while True:\n",
    "        url, endpos = get_next_link(page)\n",
    "        if url:\n",
    "            links.append(url)\n",
    "            page = page(endpos:)\n",
    "        else:\n",
    "            break\n",
    "    return links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_to_index(index, word, url):\n",
    "    if word in index:\n",
    "        index[word].append(url)\n",
    "    else:\n",
    "        index[word] = [url]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_page_to_index(index, url, content):\n",
    "    words = content.split()\n",
    "    for word in words:\n",
    "        add_to_index(index, word, url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crawler(seed):\n",
    "    tocrawl = [seed]\n",
    "    crawled = []\n",
    "    index = {}\n",
    "    graph = {}\n",
    "    while tocrawl:\n",
    "        url = tocrawl.pop\n",
    "        if url not in crawled:\n",
    "            add_page_to_index(index, url, get_page(url))\n",
    "            outlink = get_all_links(get_page(url)\n",
    "            graph[url] = outlink\n",
    "            union(tocrawl,outlink)\n",
    "            crawled.append(url)\n",
    "    return index, graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lookup(index, word):\n",
    "    if word in index:\n",
    "        return index[word]\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_ranks(graph)\n",
    "    d = 0.8 #damping constant\n",
    "    numloops = 10\n",
    "    \n",
    "    ranks = {}\n",
    "    npages = len(graph)\n",
    "    for url in graph:\n",
    "        ranks[url] = 1.0/npages\n",
    "        \n",
    "    for i in range(0, numloops):\n",
    "        newranks = {}\n",
    "        for url in graph:\n",
    "            newrank = (1-d)/npages\n",
    "            for items in graph:\n",
    "                if url in graph[items]:\n",
    "                    newrank = newrank + d * ranks[items]/len(graph[items])        \n",
    "            \n",
    "            newranks[url] = newrank\n",
    "        ranks = newranks\n",
    "    return ranks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def luck_search(index, ranks, key):\n",
    "    pages = lookup(index, key)\n",
    "    if not pages:\n",
    "        return None\n",
    "    best_page = pages[0]\n",
    "    for candidate in pages:\n",
    "        if ranks[candidate] > ranks[best_page]:\n",
    "            best_page = candidate\n",
    "    return best_page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quicksort(pages, ranks):\n",
    "    if not pages or len(pages) <= 1: \n",
    "        return pages\n",
    "    else:\n",
    "        pivot = ranks[pages[0]]\n",
    "        worse = []\n",
    "        better = []\n",
    "        for page in pages[1:]:\n",
    "            if ranks[page] <= pivot:\n",
    "                worse.append(page)\n",
    "            else:\n",
    "                better.append(page)\n",
    "        return quicksort(worse, ranks) + [pages[0]] + quicksort(better, ranks)\n",
    "\n",
    "def ordered_search(index, ranks, key):\n",
    "    pages = lookup(index,key)\n",
    "    return quicksort(pages, ranks)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
